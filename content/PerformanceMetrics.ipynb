{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will understand following concept today\n",
    "\n",
    "1.) Why model performance evaluation is required?<br>\n",
    "2.) Classification model evaluation metrics<br>\n",
    "3.) Regression model evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why model performance evaluation is required?\n",
    "\n",
    "Following are the reason to use model evaluation metrics\n",
    "\n",
    "a) We need a way to choose between  **different model types, tuning parameters, and features**\n",
    "\n",
    "b) It is used to estimate how well a model will **generalize to out-of-sample data**\n",
    "\n",
    "c) It helps to quantify the **model performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification model evaluation metrics\n",
    "\n",
    "\n",
    "### 1. Confusion Matrix\n",
    "\n",
    "A confusion matrix is summary of prediction results on classification problems.This matrix contains the following:\n",
    "\n",
    "* **TP(True positive)**: correct positive prediction\n",
    "\n",
    "* **TN(True Negative)**: correct negative prediction\n",
    "\n",
    "* **FP(False Positive)**: incorrect positive prediction\n",
    "\n",
    "* **FN(False Negative)**: incorrect negative prediction\n",
    "\n",
    "\n",
    "|   |   |   |Predicted|   |\n",
    "|---|---|---|---|---|\n",
    "|   |   |   | 1 | 0 |\n",
    "|   |Actual| 1 | TP|FN |\n",
    "|   |   | 0 | FP|TN |\n",
    "\n",
    "\n",
    "Below is the code for calculating confusion matrix for the given actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def unique_labels(*ys):\n",
    "    \"\"\"get the unique labels in the dependent variable\n",
    "    \"\"\"\n",
    "    ys_labels = set(chain.from_iterable(y for y in ys))    \n",
    "\n",
    "    return np.array(sorted(ys_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels([1,1,1,0,0,0,2,2],[0,1,1,1,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calconfusion_matrix(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix to evaluate the accuracy of a classification\n",
    "    \"\"\"    \n",
    "    \n",
    "    labels = unique_labels(y_true, y_pred)\n",
    "    \n",
    "    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)\n",
    "        \n",
    "    n_labels = labels.size\n",
    "    label_to_ind = dict((y, x) for x, y in enumerate(labels))\n",
    "    \n",
    "    # convert yt, yp into index\n",
    "    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])\n",
    "    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])\n",
    "\n",
    "    # intersect y_pred, y_true with labels, eliminate items not in labels\n",
    "    ind = np.logical_and(y_pred < n_labels, y_true < n_labels)\n",
    "    y_pred = y_pred[ind]\n",
    "    y_true = y_true[ind]\n",
    "    \n",
    "    # also eliminate weights of eliminated items\n",
    "    sample_weight = sample_weight[ind]\n",
    "\n",
    "    # Choose the accumulator dtype to always have high precision\n",
    "    if sample_weight.dtype.kind in {'i', 'u', 'b'}:\n",
    "        dtype = np.int64\n",
    "    else:\n",
    "        dtype = np.float64\n",
    "\n",
    "    CM = coo_matrix((sample_weight, (y_true, y_pred)),\n",
    "                    shape=(n_labels, n_labels), dtype=dtype,\n",
    "                    ).toarray()\n",
    "    return CM    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.array([1,1,1,0])\n",
    "y_pred=np.array([1,1,0,0])\n",
    "tn, fp, fn, tp=calconfusion_matrix(y_true,y_pred).ravel()\n",
    "p=tp+fn\n",
    "n=tn+fp\n",
    "#(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix helps to derive the following metrics\n",
    "\n",
    "* **Accuracy**: The proportion of the total number that were correct.\n",
    "    \n",
    "    $\n",
    "    ACC={\\displaystyle\\frac{TP+TN}{P+N}}        \n",
    "    $\n",
    "    \n",
    "    P=TP+FN\n",
    "    \n",
    "    N=TN+FP\n",
    "\n",
    "    Best is 1.0 and worst is 0.0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sensitivity / Recall / True Positive Rate**: Intuitively it is  the ability of the classifier to find all the positive samples \n",
    "\n",
    "    $\n",
    "    Sensitivity={\\displaystyle\\frac{TP}{TP+FN}=\\frac{TP}{P}}\n",
    "    $\n",
    "    \n",
    "    Best is 1.0 and worst is 0.0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Specificity / True Negative Rate**: The proportion of actual negative cases which are correctly identified.\n",
    "\n",
    "    $\n",
    "    Specificity={\\displaystyle\\frac{TN}{TN+FP}=\\frac{TN}{N}}\n",
    "    $\n",
    "    \n",
    "    Best is 1.0 and worst is 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Precision / Positive Predictive value**: Intuitively it is the ability of the classifier not to label as positive a sample\n",
    "    that is negative.\n",
    "\n",
    "    $\n",
    "    Precision={\\displaystyle\\frac{TP}{TP+FP}}\n",
    "    $\n",
    "    \n",
    "    Best is 1.0 and worst is 0.0\n",
    "    \n",
    "    \n",
    "* **False Positive Rate**: It is the Number of incorrect positive prediction divided by total number of negatives\n",
    "\n",
    "    $\n",
    "    FPR={\\displaystyle\\frac{FP}{TN+FP}=1-Specificity}\n",
    "    $\n",
    "\n",
    "    Best is 0.0 and worst is 1.0\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## F-Score\n",
    "\n",
    "It is a harmonic mean of Precision and Recall.\n",
    "\n",
    "### F1- score\n",
    "\n",
    "$\n",
    "F_{1}={\\displaystyle 2 \\frac{(PREC)(RECALL)}{PREC+RECALL}}\n",
    "$\n",
    "\n",
    "The score lies in the range [0,1] with 1 being ideal and 0 being the worst. Unlike the arithmetic mean, the [**harmonic mean**](https://en.wikipedia.org/wiki/Harmonic_mean) tends toward the smaller of the two elements. Hence the F1 score will be small if either precision or recall is small.\n",
    "\n",
    "### Fbeta- score\n",
    "$\n",
    "     F_{\\beta}={\\displaystyle (1+\\beta^2)\\frac{(PREC)(RECALL)}{\\beta^2(PREC+RECALL)}}\n",
    "$\n",
    "\n",
    "The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0. The **beta** parameter determines the weight of precision in the combined score.\n",
    "\n",
    "It measures the effectiveness of retrieval with respect to a user who attaches β times as much importance to recall as precision\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## MCC (Mathew Correlation coefficient)\n",
    "\n",
    "Matthews correlation coefficient is considered to be the most informative single score to establish the quality of a binary classifier prediction in a confusion matrix context.It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the **classes are of very different sizes**.\n",
    "\n",
    "$\n",
    "MCC={\\displaystyle\\frac{TP.TN-FP.FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FN)(TN+FP)}}}\n",
    "$\n",
    "\n",
    "The MCC is in essence a **correlation coefficient** between the observed and predicted binary classifications; it returns a value between **−1 and +1**. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tp,tn,p,n):\n",
    "    \"\"\"\n",
    "    calculate how accuracy of classification model    \n",
    "    \"\"\"\n",
    "    acc=np.float64(tp+tn)/np.float64(p+n)\n",
    "    return round(acc,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(tp,tn,p,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 3 out of 4 1's were correctly identified hence 75% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_recall(tp,fn):\n",
    "    \"\"\"\n",
    "    calculate the sensitivity or recall for given classification model\n",
    "    \"\"\"\n",
    "    return round(np.float64(tp)/np.float64(tp+fn),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66700000000000004"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity_recall(tp,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(tn,n):\n",
    "    \"\"\"\n",
    "    calculate the sepcificity of the classification model\n",
    "    \"\"\"\n",
    "    return round(np.float64(tn)/np.float64(n),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity(tn,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp,fp):\n",
    "    \"\"\"\n",
    "    calclulate the precision of model\n",
    "    \"\"\"\n",
    "    return np.float64(tp)/np.float64(tp+fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(tp,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FalsePositiveRate(fp,tn):\n",
    "    return np.float64(fp)/np.float64(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FalsePositiveRate(fp,tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCC(tp,tn,fp,fn):\n",
    "    \"\"\"\n",
    "    calculate the mathhew correlation coefficient\n",
    "    \"\"\"\n",
    "    return np.round(np.float64(tp*tn-fp*fn)/np.sqrt(np.float64(tp+fp)*np.float64(tp+fn)*np.float64(tn+fn)*np.float64(tn+fp)),2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57999999999999996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCC(tp,tn,fp,fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true,y_pred):\n",
    "    #calc the confusion matrix to get precision and recall\n",
    "    tn, fp, fn, tp=calconfusion_matrix(y_true,y_pred).ravel()\n",
    "    #calc precision and recall score\n",
    "    prec_score=precision(tp,fp)\n",
    "    recall_score=sensitivity_recall(tp,fn)\n",
    "    f1=2*((prec_score*recall_score)/(prec_score+recall_score))\n",
    "    return f1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score(y_true,y_pred,beta):\n",
    "    \"\"\"\n",
    "    y_true: actual y values\n",
    "    y_pred: predicted y values \n",
    "    beta: beta value passed by user to weigh the recall as imp as precision if beta==1 then fbeta_score==f1_score\n",
    "    \"\"\"\n",
    "    \n",
    "    #calc the confusion matrix to get precision and recall\n",
    "    tn, fp, fn, tp=calconfusion_matrix(y_true,y_pred).ravel()\n",
    "    #calc precision and recall score\n",
    "    prec_score=precision(tp,fp)\n",
    "    recall_score=sensitivity_recall(tp,fn)    \n",
    "    beta2=beta**2\n",
    "    fbeta_score=(1+beta2)((prec_score*recall_score)/((beta2)*(prec_score+recall_score)))\n",
    "    \n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation functions\n",
    "<a id='#Regression-Model-Evaluation-functions'> </a>\n",
    "-----\n",
    "\n",
    "**Root Mean square error**: RMSE is the square root of the average of squared errors<br>\n",
    "\n",
    "$\n",
    "RMSE={\\displaystyle \\sqrt{\\frac{\\sum_{i=1}^{n}{(Actual_{i}-Predicted_{i})^2}}{n}}}\n",
    "$\n",
    "\n",
    "\n",
    "**Mean Absolute Error**: MAE is the average absolute difference between $\\large y_{i}$ (actual) and $ \\large \\hat{y_{i}}$ (predicted values) <br>\n",
    "\n",
    "$\n",
    "MAE={\\displaystyle{\\frac{1}{n}}{\\sum_{i=1}^{n}{\\left|{y}_i-\\hat{y_i}\\right|}}}\n",
    "$\n",
    "\n",
    "**Mean Absolute percentage error**: MAPE (Mean Absolute Percent Error) measures the size of the error in percentage terms. It is calculated as the average of the unsigned percentage error <br>\n",
    "\n",
    "$\n",
    "MAPE={\\displaystyle \\frac{1}{n}{\\sum_{i=1}^{n}}  \\left| \\frac{Actual_{i}-Predicted_{i}}{Actual_{i}} \\right| *100 }\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y,y_pred):\n",
    "    \"\"\"\n",
    "    y: vector of actual values\n",
    "    y_pred: vector of predicted values\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((y-y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y,y_pred):\n",
    "    \"\"\"\n",
    "    y: vector of actual values\n",
    "    y_pred: vector of predicted values\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(y-y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y,y_pred):\n",
    "    \"\"\"\n",
    "    y: vector of actual values\n",
    "    y_pred: vector of predicted values\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs((y-y_pred)/y))*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
